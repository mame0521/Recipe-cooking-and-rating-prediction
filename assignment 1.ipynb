{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9156cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfc19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "def readCSVq1(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    c = csv.reader(f)\n",
    "    header = next(c)\n",
    "    for l in c:\n",
    "        d = dict(zip(header,l))\n",
    "        yield d['user_id'],d['recipe_id'],1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbdf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(readCSVq1(\"trainInteractions.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f3cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "train = dataset[:400000]\n",
    "valid = dataset[400000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7de1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerRecipe = defaultdict(set)\n",
    "recipesPerUser = defaultdict(set)\n",
    "\n",
    "for user,recipe,d in valid:\n",
    "    usersPerRecipe[recipe].add(user)\n",
    "    recipesPerUser[user].add(recipe)\n",
    "    \n",
    "allrecipe = set(usersPerRecipe.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de665551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add negative samples to validation set\n",
    "import random\n",
    "negative = []\n",
    "for d in valid:\n",
    "    r = allrecipe.difference(recipesPerUser[d[0]])\n",
    "    negative.append((d[0],random.sample(r,1)[0],0))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec4acf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.extend(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54646d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(predictions, y):\n",
    "\n",
    "    TP = sum([(p and l) for (p,l) in zip(predictions, y)])\n",
    "    FP = sum([(p and (1-l)) for (p,l) in zip(predictions, y)])\n",
    "    TN = sum([((1-p) and (1-l)) for (p,l) in zip(predictions, y)])\n",
    "    FN = sum([((1-p) and l) for (p,l) in zip(predictions, y)])\n",
    "    \n",
    "    acc = (TP + TN)/(TP + FP + TN + FN)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60cb3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular(threshold=0.5):\n",
    "    recipeCount = defaultdict(int)\n",
    "    totalCooked = 0\n",
    "\n",
    "    for user,recipe,_ in train:\n",
    "        recipeCount[recipe] += 1\n",
    "        totalCooked += 1\n",
    "\n",
    "    mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalCooked*threshold: break\n",
    "            \n",
    "    y_valid = [d[2] for d in valid]\n",
    "    yPred = []\n",
    "    for user,recipe,r in valid:\n",
    "        if recipe in return1:\n",
    "            yPred.append(1)\n",
    "        else:\n",
    "            yPred.append(0)\n",
    "    \n",
    "    return Accuracy(yPred,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f32cfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.598765"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular(threshold=0.71) #0.70040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27b887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d0bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipeCount = defaultdict(int)\n",
    "totalCooked = 0\n",
    "\n",
    "for user,recipe,_ in train:\n",
    "    recipeCount[recipe] += 1\n",
    "    totalCooked += 1\n",
    "\n",
    "mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalCooked*0.65: break\n",
    "\n",
    "predictions = open(\"predictions_Made_4.txt\", 'w')\n",
    "for l in open(\"stub_Made.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    if i in return1:\n",
    "        predictions.write(u + '-' + i + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + i + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc78eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerRecipe_train = defaultdict(set)\n",
    "recipesPerUser_train = defaultdict(set)\n",
    "\n",
    "for user,recipe,d in train:\n",
    "    usersPerRecipe_train[recipe].add(user)\n",
    "    recipesPerUser_train[user].add(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82b36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerRecipe = defaultdict(set)\n",
    "recipesPerUser = defaultdict(set)\n",
    "\n",
    "for user,recipe,d in dataset:\n",
    "    usersPerRecipe[recipe].add(user)\n",
    "    recipesPerUser[user].add(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41cfd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f09c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similar(threshold):\n",
    "    yPred = []\n",
    "    for u,r,_ in valid:\n",
    "        similarities = []\n",
    "        users = usersPerRecipe_train[r]\n",
    "        recipes = recipesPerUser_train[u]\n",
    "        if recipes == set():\n",
    "            yPred.append(0)\n",
    "        else:\n",
    "            for i2 in recipes:\n",
    "                if i2 == r: continue\n",
    "                sim = Jaccard(users, usersPerRecipe_train[i2])\n",
    "                similarities.append(sim)\n",
    "            if max(similarities)>threshold:\n",
    "                yPred.append(1)\n",
    "            else:\n",
    "                yPred.append(0)\n",
    "\n",
    "    return Accuracy(yPred,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7264dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_1(threshold1=0.67):\n",
    "    recipeCount = defaultdict(int)\n",
    "    totalCooked = 0\n",
    "\n",
    "    for user,recipe,_ in dataset:\n",
    "        recipeCount[recipe] += 1\n",
    "        totalCooked += 1\n",
    "\n",
    "    mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "\n",
    "    for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            return1.add(i)\n",
    "            if count > totalCooked*threshold1: break\n",
    "    \n",
    "    return return1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99a63d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = integrate_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "272c836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_test(d,threshold2=0.57):\n",
    "            \n",
    "    #yPred_1 = []\n",
    "    if d[1] in return1:\n",
    "        yPred_1=1\n",
    "    else:\n",
    "        yPred_1=0\n",
    "            \n",
    "    #yPred_2 = []\n",
    "\n",
    "    similarities = []\n",
    "    users = usersPerRecipe_train[d[1]]\n",
    "    recipes = recipesPerUser_train[d[0]]\n",
    "    if recipes == set():\n",
    "        yPred_2=0\n",
    "    else:\n",
    "        for i2 in recipes:\n",
    "            if i2 == d[1]: continue\n",
    "            sim = Jaccard(users, usersPerRecipe_train[i2])\n",
    "            similarities.append(sim)\n",
    "        if max(similarities)>threshold2:\n",
    "            yPred_2=1\n",
    "        else:\n",
    "            yPred_2=0\n",
    "                \n",
    "    yPred = yPred_1 or yPred_2\n",
    "    return(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73667b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_test_1(d,threshold2=0.55):\n",
    "            \n",
    "    #yPred_1 = []\n",
    "    if d[1] in return1:\n",
    "        yPred_1=1\n",
    "    else:\n",
    "        yPred_1=0\n",
    "            \n",
    "    #yPred_2 = []\n",
    "\n",
    "    similarities = []\n",
    "    users = usersPerRecipe[d[1]]\n",
    "    recipes = recipesPerUser[d[0]]\n",
    "    if recipes == set():\n",
    "        yPred_2=0\n",
    "    else:\n",
    "        for i2 in recipes:\n",
    "            if i2 == d[1]: continue\n",
    "            sim = Jaccard(users, usersPerRecipe[i2])\n",
    "            similarities.append(sim)\n",
    "        if max(similarities)>threshold2:\n",
    "            yPred_2=1\n",
    "        else:\n",
    "            yPred_2=0\n",
    "                \n",
    "    yPred = yPred_1 or yPred_2\n",
    "    return(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cd29560",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Made_4.txt\", 'w')\n",
    "for l in open(\"stub_Made.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + '-' + i + \",\" + str(integrate_test(d=(u,i))) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13356b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "400bf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = list(parse(\"trainRecipes.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b424985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "ingredients = []\n",
    "for d in dataset1: \n",
    "    for x in d['ingredients']:\n",
    "        ingredients.append(x)\n",
    "top50 = collections.Counter(ingredients).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d52f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856817b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e04fe555",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingsPerItem = defaultdict(set)\n",
    "itemsPerIng = defaultdict(set)\n",
    "for d in dataset1:\n",
    "    r = d['recipe_id']\n",
    "    for i in d['ingredients']:\n",
    "        ingsPerItem[r].add(i)\n",
    "        itemsPerIng[i].add(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ac05487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_test_2(d,threshold2=0.55):\n",
    "            \n",
    "    #yPred_1 = []\n",
    "    if d[1] in return1:\n",
    "        yPred_1=1\n",
    "    else:\n",
    "        yPred_1=0\n",
    "            \n",
    "    #yPred_2 = []\n",
    "\n",
    "    similarities = []\n",
    "    ings = ingsPerItem[d[1]]\n",
    "\n",
    "    for i2 in ingsPerItem:\n",
    "        if i2 == d[1]: continue\n",
    "        sim = Jaccard(ings, ingsPerItem[i2])\n",
    "        similarities.append(sim)\n",
    "    if max(similarities)>threshold2:\n",
    "        yPred_2=1\n",
    "    else:\n",
    "        yPred_2=0\n",
    "                \n",
    "    yPred = yPred_1 or yPred_2\n",
    "    return(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8db51c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Made_3.txt\", 'w')\n",
    "for l in open(\"stub_Made.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + '-' + i + \",\" + str(integrate_test_2(d=(u,i))) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e3cc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar8(i, N):\n",
    "    similarities = []\n",
    "    ings = ingsPerItem[i]\n",
    "    for i2 in ingsPerItem:\n",
    "        if i2 == i: continue\n",
    "        sim = Jaccard(ings, ingsPerItem[i2])\n",
    "        similarities.append((sim,i2))\n",
    "    similarities.sort(reverse=True)\n",
    "    similarities_dict = {list(i)[0]:[] for i in similarities}\n",
    "    for i in similarities:\n",
    "        similarities_dict[list(i)[0]].append(list(i)[1])\n",
    "    for i in similarities_dict:\n",
    "        similarities_dict[i].sort()    \n",
    "    similarities_sorted = dict(sorted(similarities_dict.items(), key=lambda x: x[0], reverse=True))\n",
    "    similarities_1 = []\n",
    "    for k,v in similarities_sorted.items():\n",
    "        for i in v:\n",
    "            similarities_1.append((k,i))\n",
    "    return [r[1] for r in similarities_1[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18648a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['68523854', '12679596', '56301588', '79675099', '87359281']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostSimilar8('06432987', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94375559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(N=5):\n",
    "    recipeCount = defaultdict(int)\n",
    "    totalCooked = 0\n",
    "\n",
    "    for user,recipe,_ in train:\n",
    "        recipeCount[recipe] += 1\n",
    "        totalCooked += 1\n",
    "\n",
    "    mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalCooked*threshold: break\n",
    "            \n",
    "    y_valid = [d[2] for d in valid]\n",
    "    yPred = []\n",
    "    for user,recipe,r in valid:\n",
    "        if recipe in return1:\n",
    "            yPred.append(1)\n",
    "        else:\n",
    "            yPred.append(0)\n",
    "    \n",
    "    return Accuracy(yPred,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bbe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------Rating Prediction-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d438570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "def readCSVq2(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    c = csv.reader(f)\n",
    "    header = next(c)\n",
    "    for l in c:\n",
    "        d = dict(zip(header,l))\n",
    "        yield d['user_id'],d['recipe_id'],int(d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d8ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = list(readCSVq2(\"trainInteractions.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe1dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset2)\n",
    "train2 = dataset2[:400000]\n",
    "valid2 = dataset2[400000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0892c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using training set for initialization\n",
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "\n",
    "for d in train2:\n",
    "    u = d[0]\n",
    "    i = d[1]\n",
    "    r = d[2]\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    itemsPerUser[u].append(i)\n",
    "    usersPerItem[i].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "981dac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = sum([r for _,_,r in train2]) / len(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dd1b48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "590d96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        if u in userIDs:\n",
    "            b_u = self.betaU[userIDs[u]]\n",
    "            g_u = self.gammaU[userIDs[u]]\n",
    "        else:\n",
    "            b_u = 0\n",
    "            g_u = 0\n",
    "            \n",
    "        if i in itemIDs:\n",
    "            b_i = self.betaI[itemIDs[i]]\n",
    "            g_i = self.gammaI[itemIDs[i]]\n",
    "        else:\n",
    "            b_i = 0\n",
    "            g_i = 0\n",
    "        \n",
    "        if (u in userIDs)&(i in itemIDs):\n",
    "            p = self.alpha + b_u + b_i + tf.tensordot(g_u, g_i, 1)\n",
    "        else:\n",
    "            p = self.alpha + b_u + b_i\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "                            tf.reduce_sum(self.betaI**2) +\\\n",
    "                            tf.reduce_sum(self.gammaU**2) +\\\n",
    "                            tf.reduce_sum(self.gammaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i +\\\n",
    "               tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dd548eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLFM = LatentFactorModel(mu, 5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4e0a8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleR = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,r = random.choice(interactions)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleR.append(r)\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1814a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 1.0506487\n",
      "iteration 20, objective = 1.2803149\n",
      "iteration 30, objective = 0.71163666\n",
      "iteration 40, objective = 0.56824106\n",
      "iteration 50, objective = 0.4817013\n",
      "iteration 60, objective = 0.4676493\n",
      "iteration 70, objective = 0.4711727\n",
      "iteration 80, objective = 0.46949345\n",
      "iteration 90, objective = 0.46731377\n",
      "iteration 100, objective = 0.4663952\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    obj = trainingStep(modelLFM, train2)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c52948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d0b24c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5690364080130236"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [r for _,_,r in train2]\n",
    "Predictions =\\\n",
    "    [modelLFM.predict(u,i).numpy() for u,i,_ in train2]\n",
    "\n",
    "print(\"MSE on training set is:\")\n",
    "MSE(Predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e149b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8729675636671943"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [r for _,_,r in valid2]\n",
    "Predictions =\\\n",
    "    [modelLFM.predict(u,i).numpy() for u,i,_ in valid2]\n",
    "\n",
    "print(\"MSE on validation set is:\")\n",
    "MSE(Predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fe5bc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test data\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "\n",
    "predictions = open(\"predictions_Rated_1.txt\", 'w')\n",
    "for l in open(\"stub_Rated.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + '-' + i + ',' + str(modelLFM.predict(u,i).numpy()) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple LFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c4634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d5ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "\n",
    "for d in dataset2:\n",
    "    u = d[0]\n",
    "    i = d[1]\n",
    "    r = d[2]\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    itemsPerUser[u].append(i)\n",
    "    usersPerItem[i].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7b3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset2)\n",
    "nUsers = len(userIDs)\n",
    "nItems = len(itemIDs)\n",
    "users = list(userIDs.keys())\n",
    "items = list(itemIDs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea128de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sum([r for _,_,r in dataset2]) / len(dataset2)\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327f8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768f9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b504d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d[0], d[1]) for d in dataset2]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72e0747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset2)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in dataset2:\n",
    "        u,i,r = d[0], d[1], d[2]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - r\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "532c0088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.900892329560429\n",
      "MSE = 0.912094139199284\n",
      "MSE = 0.8986029223746386\n",
      "MSE = 0.893968039632259\n",
      "MSE = 0.8783901141972499\n",
      "MSE = 0.8773597977351407\n",
      "MSE = 0.8759599169505846\n",
      "MSE = 0.8735764547410022\n",
      "MSE = 0.868335939410115\n",
      "MSE = 0.8619473966465476\n",
      "MSE = 0.849562336368106\n",
      "MSE = 0.8386945366257139\n",
      "MSE = 0.8375236780071237\n",
      "MSE = 0.8364364529251572\n",
      "MSE = 0.8313225566399429\n",
      "MSE = 0.8243140241313085\n",
      "MSE = 0.8189316210240377\n",
      "MSE = 0.8180430155798404\n",
      "MSE = 0.8169685813046845\n",
      "MSE = 0.8144613858096241\n",
      "MSE = 0.8041674118028445\n",
      "MSE = 0.8015156997439964\n",
      "MSE = 0.7985761191419435\n",
      "MSE = 0.7964273085078105\n",
      "MSE = 0.7930529679799972\n",
      "MSE = 0.7891658933504208\n",
      "MSE = 0.7789856826543706\n",
      "MSE = 0.7733208742969913\n",
      "MSE = 0.7713988242121539\n",
      "MSE = 0.7700356251466455\n",
      "MSE = 0.7692798896699778\n",
      "MSE = 0.767791692775971\n",
      "MSE = 0.7640478049340007\n",
      "MSE = 0.7569499342045326\n",
      "MSE = 0.7561009234534161\n",
      "MSE = 0.755834186408082\n",
      "MSE = 0.7531796810728038\n",
      "MSE = 1.0498660121222496\n",
      "MSE = 0.7518246155128584\n",
      "MSE = 0.7450130655523\n",
      "MSE = 0.7501433977830473\n",
      "MSE = 0.7444260743923846\n",
      "MSE = 0.7429891557903432\n",
      "MSE = 0.7420938402522015\n",
      "MSE = 0.7422679016234826\n",
      "MSE = 0.7413204751635368\n",
      "MSE = 0.7378445422802042\n",
      "MSE = 0.7356483575534881\n",
      "MSE = 0.7357276073844975\n",
      "MSE = 0.7359000818994078\n",
      "MSE = 0.7353302411829846\n",
      "MSE = 0.7488160082104783\n",
      "MSE = 0.7356663270180611\n",
      "MSE = 0.7349472981270966\n",
      "MSE = 0.734019211307582\n",
      "MSE = 0.7324613610390341\n",
      "MSE = 0.7310683371282093\n",
      "MSE = 0.7290807890176809\n",
      "MSE = 0.7289948047227585\n",
      "MSE = 0.7285902852852724\n",
      "MSE = 0.7291341948246308\n",
      "MSE = 0.7265334430311866\n",
      "MSE = 0.7205121989578833\n",
      "MSE = 0.7235122745190808\n",
      "MSE = 0.720900745537019\n",
      "MSE = 0.7209820364573993\n",
      "MSE = 0.7210009083916373\n",
      "MSE = 0.7208765728065122\n",
      "MSE = 0.7200436810450325\n",
      "MSE = 0.7185153278278775\n",
      "MSE = 0.716260659403668\n",
      "MSE = 0.7156089064583864\n",
      "MSE = 0.7153361189277716\n",
      "MSE = 0.713872628363432\n",
      "MSE = 0.7118736015143344\n",
      "MSE = 0.7106515172840376\n",
      "MSE = 0.710155903532753\n",
      "MSE = 0.710203715993276\n",
      "MSE = 0.71041938504995\n",
      "MSE = 0.7106308998044728\n",
      "MSE = 0.7168025821617036\n",
      "MSE = 0.710594145147904\n",
      "MSE = 0.7102638125810029\n",
      "MSE = 0.7097257610899641\n",
      "MSE = 0.7091964926347429\n",
      "MSE = 0.7089929866805443\n",
      "MSE = 0.7079358238116542\n",
      "MSE = 0.7042985423690246\n",
      "MSE = 0.7073543782573725\n",
      "MSE = 0.7063550506955607\n",
      "MSE = 0.7066201936683165\n",
      "MSE = 0.7070963605317181\n",
      "MSE = 0.7071520415386245\n",
      "MSE = 0.7069704264948713\n",
      "MSE = 0.7065588308187138\n",
      "MSE = 0.7064590960276338\n",
      "MSE = 0.7064866292470716\n",
      "MSE = 0.7066706714223175\n",
      "MSE = 0.7070972669903728\n",
      "MSE = 0.7062543522765761\n",
      "MSE = 0.7064352696299028\n",
      "MSE = 0.7064643325611776\n",
      "MSE = 0.706508285266824\n",
      "MSE = 0.7066327834816893\n",
      "MSE = 0.7108930511663556\n",
      "MSE = 0.7065204699081099\n",
      "MSE = 0.7064792115695389\n",
      "MSE = 0.7063373923354006\n",
      "MSE = 0.7062214386146742\n",
      "MSE = 0.7060921877753167\n",
      "MSE = 0.7061636394149489\n",
      "MSE = 0.7067252873403321\n",
      "MSE = 0.7061839934117495\n",
      "MSE = 0.7063116023964631\n",
      "MSE = 0.7061352775744255\n",
      "MSE = 0.7062211315918521\n",
      "MSE = 0.7063333208680724\n",
      "MSE = 0.7063808004098764\n",
      "MSE = 0.7062317658449643\n",
      "MSE = 0.7060548717767416\n",
      "MSE = 0.7060045236552397\n",
      "MSE = 0.705513657199268\n",
      "MSE = 0.7053162520044358\n",
      "MSE = 0.7048412467824083\n",
      "MSE = 0.7122948720829423\n",
      "MSE = 0.7048371563365313\n",
      "MSE = 0.704638812368469\n",
      "MSE = 0.705059259710164\n",
      "MSE = 0.7046476047512984\n",
      "MSE = 0.7046409084848702\n",
      "MSE = 0.7046645720382864\n",
      "MSE = 0.704641280184487\n",
      "MSE = 0.7045154991502577\n",
      "MSE = 0.7043461964845581\n",
      "MSE = 0.7042172574039709\n",
      "MSE = 0.7041705423283361\n",
      "MSE = 0.7042169797320925\n",
      "MSE = 0.7041066777248403\n",
      "MSE = 0.7039733665868015\n",
      "MSE = 0.7038836810887933\n",
      "MSE = 0.7042108265686143\n",
      "MSE = 0.7038655417972699\n",
      "MSE = 0.7037731176452906\n",
      "MSE = 0.7036870737537901\n",
      "MSE = 0.7036202993517\n",
      "MSE = 0.7035703780337244\n",
      "MSE = 0.7035061649211737\n",
      "MSE = 0.703319366651041\n",
      "MSE = 0.703423419702806\n",
      "MSE = 0.7033334195788078\n",
      "MSE = 0.7033923938799582\n",
      "MSE = 0.70337734918431\n",
      "MSE = 0.703353842245643\n",
      "MSE = 0.7032888853689476\n",
      "MSE = 0.7032265251145705\n",
      "MSE = 0.7032506259165409\n",
      "MSE = 0.7031271869511989\n",
      "MSE = 0.7030348420984118\n",
      "MSE = 0.702933645641394\n",
      "MSE = 0.7024148093318798\n",
      "MSE = 0.7028305817998738\n",
      "MSE = 0.7026695754569782\n",
      "MSE = 0.7025973649355962\n",
      "MSE = 0.7023570140274936\n",
      "MSE = 0.7024990005502674\n",
      "MSE = 0.7023329479949258\n",
      "MSE = 0.7016751315631351\n",
      "MSE = 0.7022151107931484\n",
      "MSE = 0.7021283480582976\n",
      "MSE = 0.701635514576258\n",
      "MSE = 0.7020593479888115\n",
      "MSE = 0.7020955239329224\n",
      "MSE = 0.702098087517992\n",
      "MSE = 0.7017175986724222\n",
      "MSE = 0.7020381434437579\n",
      "MSE = 0.7019907164992811\n",
      "MSE = 0.701998257888806\n",
      "MSE = 0.7019267405786679\n",
      "MSE = 0.7019536219610828\n",
      "MSE = 0.7019222523236928\n",
      "MSE = 0.7017871083573659\n",
      "MSE = 0.7018236048872855\n",
      "MSE = 0.7017879992292803\n",
      "MSE = 0.701680624897705\n",
      "MSE = 0.7015833598937158\n",
      "MSE = 0.7015534190282213\n",
      "MSE = 0.7010650366910113\n",
      "MSE = 0.7014902996664908\n",
      "MSE = 0.7014888844777636\n",
      "MSE = 0.7013774801668158\n",
      "MSE = 0.7014296803475355\n",
      "MSE = 0.701451930037905\n",
      "MSE = 0.7014574858508021\n",
      "MSE = 0.7012932296984078\n",
      "MSE = 0.7014339003527091\n",
      "MSE = 0.7013451291082796\n",
      "MSE = 0.7025535225557632\n",
      "MSE = 0.7013520360918786\n",
      "MSE = 0.701345796572197\n",
      "MSE = 0.7013452001203994\n",
      "MSE = 0.7013451367379873\n",
      "MSE = 0.7013451299293137\n",
      "MSE = 0.7013451291972519\n",
      "MSE = 0.7013451291187038\n",
      "MSE = 0.7013451291097584\n",
      "MSE = 0.7013451291156337\n",
      "MSE = 0.7013451291107746\n",
      "MSE = 0.7013451291098154\n",
      "MSE = 0.7013451291098168\n",
      "MSE = 0.7013451291098154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.45992407, -0.05264485,  0.29908444, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " 0.7508028966003583,\n",
       " {'grad': array([-2.33960649e-04,  2.13222023e-07,  5.95725680e-06, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
       "  'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 210,\n",
       "  'nit': 167,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [r for _,_,r in dataset2]\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 0.000018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d64cd8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.900892329560429\n",
      "MSE = 0.912094139199284\n",
      "MSE = 0.8986029140348785\n",
      "MSE = 0.8939680414190727\n",
      "MSE = 0.8783903853581955\n",
      "MSE = 0.8773603492250993\n",
      "MSE = 0.875958568664379\n",
      "MSE = 0.8735732618510075\n",
      "MSE = 0.868329483898093\n",
      "MSE = 0.8619417586923738\n",
      "MSE = 0.8495570600376476\n",
      "MSE = 0.8386894869435184\n",
      "MSE = 0.8375191422895973\n",
      "MSE = 0.8364310883615965\n",
      "MSE = 0.8312962895793983\n",
      "MSE = 0.824601347159106\n",
      "MSE = 0.818979252733498\n",
      "MSE = 0.8180921753971528\n",
      "MSE = 0.8171254249466019\n",
      "MSE = 0.8290933178323358\n",
      "MSE = 0.8164277081500989\n",
      "MSE = 0.8121110107979523\n",
      "MSE = 0.8056262550499482\n",
      "MSE = 0.8012948218687105\n",
      "MSE = 0.7988894172135668\n",
      "MSE = 0.798385745496589\n",
      "MSE = 0.7958669478656714\n",
      "MSE = 0.7882712036275048\n",
      "MSE = 0.7860225925645399\n",
      "MSE = 0.7853296964654692\n",
      "MSE = 0.7843426868587458\n",
      "MSE = 0.7838684232921839\n",
      "MSE = 0.7833461159577079\n",
      "MSE = 0.7817737268411941\n",
      "MSE = 0.7784799273963352\n",
      "MSE = 0.7740531023764013\n",
      "MSE = 0.7689253104160394\n",
      "MSE = 0.7670426216741915\n",
      "MSE = 0.7659696130691561\n",
      "MSE = 0.7636827642622666\n",
      "MSE = 0.7614624769180556\n",
      "MSE = 0.760149919960056\n",
      "MSE = 0.7577852346452276\n",
      "MSE = 0.7532102807251132\n",
      "MSE = 3.0144802072562755\n",
      "MSE = 0.7520003788405457\n",
      "MSE = 0.7486829921178938\n",
      "MSE = 0.7470918533427405\n",
      "MSE = 0.7426951757713776\n",
      "MSE = 0.8906056882635565\n",
      "MSE = 0.7425889823204214\n",
      "MSE = 0.7401935462754997\n",
      "MSE = 0.7390755013299634\n",
      "MSE = 0.7385394701571801\n",
      "MSE = 0.735966816035116\n",
      "MSE = 0.7405613737184004\n",
      "MSE = 0.7354404274130067\n",
      "MSE = 0.734601471543979\n",
      "MSE = 0.7311724823011858\n",
      "MSE = 0.7290009230361507\n",
      "MSE = 0.7287771628037252\n",
      "MSE = 0.7280701346556063\n",
      "MSE = 0.7251415664091512\n",
      "MSE = 0.7236398463369597\n",
      "MSE = 0.7217810555152907\n",
      "MSE = 0.7202862842357369\n",
      "MSE = 0.7194960856435095\n",
      "MSE = 0.7181066251353003\n",
      "MSE = 0.7178390384233561\n",
      "MSE = 0.7163347513762534\n",
      "MSE = 0.7168302462923325\n",
      "MSE = 0.7171331643758656\n",
      "MSE = 0.7173284792133804\n",
      "MSE = 0.7155866748518711\n",
      "MSE = 0.7627267542975811\n",
      "MSE = 0.7157532073541917\n",
      "MSE = 0.7149566317535471\n",
      "MSE = 0.715290088634397\n",
      "MSE = 0.7141095523947761\n",
      "MSE = 0.7144657405975959\n",
      "MSE = 0.7143635101114695\n",
      "MSE = 0.7143684753657452\n",
      "MSE = 0.714291480741302\n",
      "MSE = 0.713949457873183\n",
      "MSE = 0.7135300280762726\n",
      "MSE = 0.7146781952216963\n",
      "MSE = 0.7133512943664593\n",
      "MSE = 0.7128134529098388\n",
      "MSE = 0.7125146890702104\n",
      "MSE = 0.7123945769717108\n",
      "MSE = 0.7123303764095572\n",
      "MSE = 0.7122857269238864\n",
      "MSE = 0.7121349295702895\n",
      "MSE = 0.7120134560851992\n",
      "MSE = 0.7118106846311608\n",
      "MSE = 0.7112722738962777\n",
      "MSE = 0.7103405570866995\n",
      "MSE = 0.7102959166346805\n",
      "MSE = 0.7101997780811009\n",
      "MSE = 0.709930404721539\n",
      "MSE = 0.7096218407294657\n",
      "MSE = 0.7094962176282665\n",
      "MSE = 0.7091048545829549\n",
      "MSE = 0.7085890164089234\n",
      "MSE = 0.7078525052251582\n",
      "MSE = 0.7082687028623726\n",
      "MSE = 0.707933667500015\n",
      "MSE = 0.7075204802769706\n",
      "MSE = 0.7067657450390913\n",
      "MSE = 0.7064565672127844\n",
      "MSE = 0.7061044630916811\n",
      "MSE = 0.7060121750492141\n",
      "MSE = 0.7058986164526577\n",
      "MSE = 0.7055722424217613\n",
      "MSE = 0.7051058394449674\n",
      "MSE = 0.7051122576282497\n",
      "MSE = 0.70507319394176\n",
      "MSE = 0.7048196983923322\n",
      "MSE = 0.7042456502965229\n",
      "MSE = 0.7040614315926627\n",
      "MSE = 0.7039052103297573\n",
      "MSE = 0.7033109248095613\n",
      "MSE = 0.7042862626753266\n",
      "MSE = 0.7035984262036965\n",
      "MSE = 0.7032126607622561\n",
      "MSE = 0.7030050889753419\n",
      "MSE = 0.7029037097986361\n",
      "MSE = 0.7029146895017927\n",
      "MSE = 0.7028855989583928\n",
      "MSE = 0.7035060164180673\n",
      "MSE = 0.7028930070267607\n",
      "MSE = 0.7028392782907503\n",
      "MSE = 0.7029704954706842\n",
      "MSE = 0.7028232086567263\n",
      "MSE = 0.7026827229196685\n",
      "MSE = 0.7025564903956193\n",
      "MSE = 0.7023632005767453\n",
      "MSE = 0.7022121268912841\n",
      "MSE = 0.7018971890689082\n",
      "MSE = 0.7018478906056277\n",
      "MSE = 0.7018003438181418\n",
      "MSE = 0.7018412784343256\n",
      "MSE = 0.7028232076103541\n",
      "MSE = 0.7018493173265592\n",
      "MSE = 0.7018755788218881\n",
      "MSE = 0.7018976160378563\n",
      "MSE = 0.7018744314595292\n",
      "MSE = 0.7017529344274261\n",
      "MSE = 0.7016904584296847\n",
      "MSE = 0.7016246923529614\n",
      "MSE = 0.701512087487428\n",
      "MSE = 0.7013203095360466\n",
      "MSE = 0.7010969988423453\n",
      "MSE = 0.7010634490432273\n",
      "MSE = 0.701050030559121\n",
      "MSE = 0.7010319124889521\n",
      "MSE = 0.701047274631487\n",
      "MSE = 0.7010340151518538\n",
      "MSE = 0.7010133669034779\n",
      "MSE = 0.7009682571610193\n",
      "MSE = 0.7006406757510266\n",
      "MSE = 0.7006767647311416\n",
      "MSE = 0.7007196963466612\n",
      "MSE = 0.7007275684202247\n",
      "MSE = 0.7010064186921269\n",
      "MSE = 0.7007686800273076\n",
      "MSE = 0.7007116180440827\n",
      "MSE = 0.7007356572611173\n",
      "MSE = 0.7007112094265229\n",
      "MSE = 0.7007115456067462\n",
      "MSE = 0.7007116072295744\n",
      "MSE = 0.7007116164570568\n",
      "MSE = 0.7007116178116316\n",
      "MSE = 0.7007116180100204\n",
      "MSE = 0.7007116180390843\n",
      "MSE = 0.7007116180432965\n",
      "MSE = 0.7007116180439557\n",
      "MSE = 0.700711618044016\n",
      "MSE = 0.7007116180439708\n",
      "MSE = 0.7007116180439956\n",
      "MSE = 0.7007116180439911\n",
      "MSE = 0.7007116180440007\n",
      "MSE = 0.7007116180439958\n",
      "MSE = 0.7007116180439971\n",
      "MSE = 0.7007116180439958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.45830346, -0.05398531,  0.30300362, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " 0.7500398451836868,\n",
       " {'grad': array([-9.65742874e-04,  3.33521307e-08,  1.79969503e-05, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
       "  'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 185,\n",
       "  'nit': 145,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [r for _,_,r in dataset2]\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 0.0000177))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ca2560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9177020321053291\n",
      "MSE = 1.6544952869696699\n",
      "MSE = 0.9007262123143708\n",
      "MSE = 0.9005928841836659\n",
      "MSE = 0.9000639254762641\n",
      "MSE = 0.8980177516939493\n",
      "MSE = 0.8909476333213814\n",
      "MSE = 0.8642956843298382\n",
      "MSE = 0.847208769496609\n",
      "MSE = 0.8313734912948623\n",
      "MSE = 0.8147823798318572\n",
      "MSE = 0.8077895805035806\n",
      "MSE = 0.7927502431126554\n",
      "MSE = 0.7779238274165918\n",
      "MSE = 0.7635799010412583\n",
      "MSE = 0.7571727890163625\n",
      "MSE = 0.7473265437014849\n",
      "MSE = 0.7365949636724648\n",
      "MSE = 0.8635218207043825\n",
      "MSE = 0.7360629281840232\n",
      "MSE = 0.7345712797437509\n",
      "MSE = 0.7294860599557363\n",
      "MSE = 0.726294397330295\n",
      "MSE = 0.7214162047281146\n",
      "MSE = 0.7151831146172776\n",
      "MSE = 0.7110853233913704\n",
      "MSE = 0.7115523540545712\n",
      "MSE = 0.7065429167950928\n",
      "MSE = 0.7040127734927725\n",
      "MSE = 0.7011089538493986\n",
      "MSE = 0.6962253647083416\n",
      "MSE = 0.6981170189785076\n",
      "MSE = 0.7128819024162811\n",
      "MSE = 0.6980198973237216\n",
      "MSE = 0.6968163756806419\n",
      "MSE = 0.6947268302844494\n",
      "MSE = 0.6934639234551158\n",
      "MSE = 0.691848841569688\n",
      "MSE = 0.6894191026073316\n",
      "MSE = 0.6866394499807515\n",
      "MSE = 0.6855561380924938\n",
      "MSE = 0.685892324875926\n",
      "MSE = 0.6861302226742862\n",
      "MSE = 0.6852581122490652\n",
      "MSE = 0.6854640770380719\n",
      "MSE = 0.685182021778887\n",
      "MSE = 0.6848972976347928\n",
      "MSE = 0.6843469909642251\n",
      "MSE = 0.6839790141839311\n",
      "MSE = 0.683818759575158\n",
      "MSE = 0.6835416961685963\n",
      "MSE = 0.6833366465317297\n",
      "MSE = 0.6817691532889729\n",
      "MSE = 0.6826257650148478\n",
      "MSE = 0.6827066406114236\n",
      "MSE = 0.6829571487748353\n",
      "MSE = 0.683016647708804\n",
      "MSE = 0.6830692551317833\n",
      "MSE = 0.6829319871869997\n",
      "MSE = 0.6828389335024775\n",
      "MSE = 0.68276848096799\n",
      "MSE = 0.6827223833647399\n",
      "MSE = 0.6827745088042924\n",
      "MSE = 0.686040021942923\n",
      "MSE = 0.6827309346335504\n",
      "MSE = 0.6828431878145763\n",
      "MSE = 0.6829464645850144\n",
      "MSE = 0.6829672616753654\n",
      "MSE = 0.6830460429742785\n",
      "MSE = 0.6831047151966382\n",
      "MSE = 0.6831379377302214\n",
      "MSE = 0.6835319046490596\n",
      "MSE = 0.6831953746216327\n",
      "MSE = 0.6831294564005186\n",
      "MSE = 0.683164048872198\n",
      "MSE = 0.6832065258153478\n",
      "MSE = 0.6832282577421109\n",
      "MSE = 0.6840227868026915\n",
      "MSE = 0.6832420534636959\n",
      "MSE = 0.6832875081118834\n",
      "MSE = 0.6833203838173024\n",
      "MSE = 0.6833128652725756\n",
      "MSE = 0.683405770783801\n",
      "MSE = 0.6833354320946441\n",
      "MSE = 0.683254712221547\n",
      "MSE = 0.6832989586855054\n",
      "MSE = 0.6832673246731734\n",
      "MSE = 0.6832610185707171\n",
      "MSE = 0.6832305829425984\n",
      "MSE = 0.6831587762423802\n",
      "MSE = 0.6831454197257553\n",
      "MSE = 0.6831537996122514\n",
      "MSE = 0.6831467721141614\n",
      "MSE = 0.6831455216841547\n",
      "MSE = 0.6831430107987906\n",
      "MSE = 0.6831196347888396\n",
      "MSE = 0.682498291298528\n",
      "MSE = 0.6830851930689357\n",
      "MSE = 0.6830541524850055\n",
      "MSE = 0.6829652360607176\n",
      "MSE = 0.6830479329537896\n",
      "MSE = 0.6830218666745342\n",
      "MSE = 0.6830633439749754\n",
      "MSE = 0.6830286355985954\n",
      "MSE = 0.6830082345768288\n",
      "MSE = 0.6829552589544252\n",
      "MSE = 0.6829321507277893\n",
      "MSE = 0.6829031315331406\n",
      "MSE = 0.682911332035933\n",
      "MSE = 0.6829120819724807\n",
      "MSE = 0.68289635782527\n",
      "MSE = 0.6829261889961417\n",
      "MSE = 0.6829007813623226\n",
      "MSE = 0.6829135829971039\n",
      "MSE = 0.6828948550275681\n",
      "MSE = 0.6828794163776958\n",
      "MSE = 0.6828372297389202\n",
      "MSE = 0.6828139646499788\n",
      "MSE = 0.6827906475644367\n",
      "MSE = 0.6827395217796611\n",
      "MSE = 0.6827353335284848\n",
      "MSE = 0.6827628706113015\n",
      "MSE = 0.6827713668911927\n",
      "MSE = 0.6827701867094235\n",
      "MSE = 0.6827321364643117\n",
      "MSE = 0.6827195442639955\n",
      "MSE = 0.6827081016251916\n",
      "MSE = 0.6826780498122411\n",
      "MSE = 0.6824044539315038\n",
      "MSE = 0.6826553819725426\n",
      "MSE = 0.6826283295805501\n",
      "MSE = 0.6826022502783842\n",
      "MSE = 0.6825837508822235\n",
      "MSE = 0.6825612227624361\n",
      "MSE = 0.6825456727100689\n",
      "MSE = 0.6825430047136972\n",
      "MSE = 0.6825343378020883\n",
      "MSE = 0.6825359724665077\n",
      "MSE = 0.6825345617043472\n",
      "MSE = 0.6825336607773882\n",
      "MSE = 0.6825282176592218\n",
      "MSE = 0.6825174295277967\n",
      "MSE = 0.6824784961884269\n",
      "MSE = 0.6824628583719476\n",
      "MSE = 0.682475171331161\n",
      "MSE = 0.6824496139038552\n",
      "MSE = 0.6824419278465341\n",
      "MSE = 0.6824354350018544\n",
      "MSE = 0.6824046992135506\n",
      "MSE = 0.6824276372562325\n",
      "MSE = 0.6824167801545684\n",
      "MSE = 0.6824113209786302\n",
      "MSE = 0.682389337118371\n",
      "MSE = 0.6824077503121617\n",
      "MSE = 0.6824090020466588\n",
      "MSE = 0.6824081457348865\n",
      "MSE = 0.6823970594245128\n",
      "MSE = 0.6823836865587087\n",
      "MSE = 0.6824023217292338\n",
      "MSE = 0.6823850238962149\n",
      "MSE = 0.6823651368138113\n",
      "MSE = 0.6823311824855678\n",
      "MSE = 0.6823260724120341\n",
      "MSE = 0.6823293020223488\n",
      "MSE = 0.6823110430063244\n",
      "MSE = 0.6823090321392621\n",
      "MSE = 0.6823018922648981\n",
      "MSE = 0.6823076613494441\n",
      "MSE = 0.6823083240064003\n",
      "MSE = 0.6823086714986913\n",
      "MSE = 0.6823103569108234\n",
      "MSE = 0.6823114381780124\n",
      "MSE = 0.682315346492449\n",
      "MSE = 0.6823121532210286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.45184331,  0.03789561, -0.11018859, ...,  0.0852448 ,\n",
       "         0.03788154,  0.01467725]),\n",
       " 0.7393261451925227,\n",
       " {'grad': array([ 4.96913404e-06,  8.82376342e-08,  1.63835054e-08, ...,\n",
       "         -2.07550097e-08, -7.61874525e-09, -8.54431091e-09]),\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 174,\n",
       "  'nit': 146,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [r for _,_,r in dataset2]\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 0.000019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41bf9bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9180114411884044\n",
      "MSE = 1.652503818524502\n",
      "MSE = 0.9007259253622535\n",
      "MSE = 0.900592609218318\n",
      "MSE = 0.9000636976612182\n",
      "MSE = 0.8980176997341913\n",
      "MSE = 0.8909480808588485\n",
      "MSE = 0.8642934323002804\n",
      "MSE = 0.8472058203736941\n",
      "MSE = 0.8313876969506002\n",
      "MSE = 0.8147768661212363\n",
      "MSE = 0.8077627000077132\n",
      "MSE = 0.7927018396108559\n",
      "MSE = 0.7778628602954507\n",
      "MSE = 0.7634776346971955\n",
      "MSE = 0.7570174918120194\n",
      "MSE = 0.747084616268834\n",
      "MSE = 0.7363289003594129\n",
      "MSE = 0.8648149029607639\n",
      "MSE = 0.7358002944542071\n",
      "MSE = 0.7343081292518157\n",
      "MSE = 0.7289711725213355\n",
      "MSE = 0.7258068846974075\n",
      "MSE = 0.7206148140366679\n",
      "MSE = 0.7143289995138398\n",
      "MSE = 0.7076976163688928\n",
      "MSE = 0.702286177709134\n",
      "MSE = 0.7020147267795441\n",
      "MSE = 0.7003165621603097\n",
      "MSE = 0.6948326770631181\n",
      "MSE = 0.720517947778356\n",
      "MSE = 0.6947128562602398\n",
      "MSE = 0.6942524094115656\n",
      "MSE = 0.6917909092277517\n",
      "MSE = 0.6909141261339704\n",
      "MSE = 0.6888211759645867\n",
      "MSE = 0.6877026889643679\n",
      "MSE = 0.6863346074259057\n",
      "MSE = 0.6866928491798835\n",
      "MSE = 0.6869882878047638\n",
      "MSE = 0.6865529769592055\n",
      "MSE = 0.683295025153558\n",
      "MSE = 0.6889838116531283\n",
      "MSE = 0.6832555482191689\n",
      "MSE = 0.6839452779465229\n",
      "MSE = 0.6834080392873837\n",
      "MSE = 0.6825337608136228\n",
      "MSE = 0.6819380555745556\n",
      "MSE = 0.6811129311909115\n",
      "MSE = 0.6811230963097739\n",
      "MSE = 0.6814581543973309\n",
      "MSE = 0.6813413400279096\n",
      "MSE = 0.6815041226924228\n",
      "MSE = 0.681860372576709\n",
      "MSE = 0.6971777657545953\n",
      "MSE = 0.6818642997537977\n",
      "MSE = 0.6819289884936157\n",
      "MSE = 0.6838585439347927\n",
      "MSE = 0.6823490601302986\n",
      "MSE = 0.6823038259263936\n",
      "MSE = 0.6827244174598507\n",
      "MSE = 0.6826510519288299\n",
      "MSE = 0.6825223091818026\n",
      "MSE = 0.6822463235121697\n",
      "MSE = 0.6820529664297168\n",
      "MSE = 0.6811095118613171\n",
      "MSE = 0.6812458247091292\n",
      "MSE = 0.6812270901604841\n",
      "MSE = 0.681160313026866\n",
      "MSE = 0.6812138341121978\n",
      "MSE = 0.6811303863892653\n",
      "MSE = 0.6810137346762306\n",
      "MSE = 0.6809170321698268\n",
      "MSE = 0.6807666183607533\n",
      "MSE = 0.6806770117119316\n",
      "MSE = 0.6807083397953471\n",
      "MSE = 0.6807623552905603\n",
      "MSE = 0.6807449432330985\n",
      "MSE = 0.6807044835303867\n",
      "MSE = 0.6806790042811174\n",
      "MSE = 0.6806413343975032\n",
      "MSE = 0.6806677532032324\n",
      "MSE = 0.6806649428469473\n",
      "MSE = 0.6806549059156554\n",
      "MSE = 0.6806448758024023\n",
      "MSE = 0.6806257883992368\n",
      "MSE = 0.6806677633559675\n",
      "MSE = 0.6806471689792295\n",
      "MSE = 0.6806095250544053\n",
      "MSE = 0.6805976192946782\n",
      "MSE = 0.6805757584495397\n",
      "MSE = 0.6805488732549707\n",
      "MSE = 0.6805259656285976\n",
      "MSE = 0.6804479666644403\n",
      "MSE = 0.680523218884951\n",
      "MSE = 0.6805187489854628\n",
      "MSE = 0.6804913884880581\n",
      "MSE = 0.680513995116545\n",
      "MSE = 0.6805179573568413\n",
      "MSE = 0.6805261383342054\n",
      "MSE = 0.6805090052592417\n",
      "MSE = 0.6805371368060387\n",
      "MSE = 0.6805392272455602\n",
      "MSE = 0.680551437432658\n",
      "MSE = 0.6805527761833307\n",
      "MSE = 0.6805980566657202\n",
      "MSE = 0.6806903539481578\n",
      "MSE = 0.6805994572382114\n",
      "MSE = 0.6805761465689605\n",
      "MSE = 0.680690173220225\n",
      "MSE = 0.6805795176349991\n",
      "MSE = 0.6805764610998241\n",
      "MSE = 0.68057834088807\n",
      "MSE = 0.6805848980216653\n",
      "MSE = 0.6806179355797458\n",
      "MSE = 0.6806010855725066\n",
      "MSE = 0.6806004833407481\n",
      "MSE = 0.6806011074232295\n",
      "MSE = 0.6806021243505108\n",
      "MSE = 0.680604799199533\n",
      "MSE = 0.6806146088808771\n",
      "MSE = 0.6806184055153439\n",
      "MSE = 0.6806162551152565\n",
      "MSE = 0.6806168963496625\n",
      "MSE = 0.6806180094732102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.45114176,  0.03821684, -0.11134527, ...,  0.08725998,\n",
       "         0.03874497,  0.01505306]),\n",
       " 0.738106351374945,\n",
       " {'grad': array([-6.48261993e-06, -7.47459480e-08,  2.36366345e-09, ...,\n",
       "         -1.02124836e-08, -1.99873043e-09, -1.65557699e-09]),\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 125,\n",
       "  'nit': 106,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [r for _,_,r in dataset2]\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 0.0000185))#current best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad061d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFM_pred(u,i):\n",
    "    if u in userIDs:\n",
    "        b_u = userBiases[u]\n",
    "    else:\n",
    "        b_u = 0\n",
    "            \n",
    "    if i in itemIDs:\n",
    "        b_i = itemBiases[i]\n",
    "    else:\n",
    "        b_i = 0\n",
    "\n",
    "    p = alpha + b_u + b_i        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2da9fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8124466463070734"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_valid = [r for _,_,r in valid2]\n",
    "pred_valid = [LFM_pred(u,i) for u,i,_ in valid2]\n",
    "\n",
    "print(\"MSE on validation set is:\")\n",
    "MSE(pred_valid, label_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac8e1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rated_7.txt\", 'w')\n",
    "for l in open(\"stub_Rated.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + '-' + i + ',' + str(LFM_pred(u,i)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f4d752cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sum([r for _,_,r in dataset2]) / len(dataset2)\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "userGamma = {}\n",
    "itemGamma = {}\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "30576496",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in userIDs:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f00b3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itemIDs:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bac8e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index:index+nUsers]))\n",
    "    index += nUsers\n",
    "    itemBiases = dict(zip(items, theta[index:index+nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index+K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index+K]\n",
    "        index += K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "237a3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1c134178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item] + inner(userGamma[user], itemGamma[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1568b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d[0], d[1]) for d in dataset2]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*itemGamma[i][k]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1a0c6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in userIDs:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in itemIDs:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for d in dataset2:\n",
    "        u,i,r = d[0], d[1], d[2]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - r\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "            dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9a143df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9080434122995151\n",
      "MSE = 1.7294306228010499\n",
      "MSE = 0.9007388227126125\n",
      "MSE = 0.9006046665398597\n",
      "MSE = 0.9000724498355834\n",
      "MSE = 0.8980141108064839\n",
      "MSE = 0.890909199278654\n",
      "MSE = 0.8643288543476435\n",
      "MSE = 0.847303433607903\n",
      "MSE = 0.8310509550674388\n",
      "MSE = 0.8150932070561523\n",
      "MSE = 0.8091408928072984\n",
      "MSE = 0.7956434915046718\n",
      "MSE = 0.782785648160797\n",
      "MSE = 0.771192266664044\n",
      "MSE = 0.7674168654692667\n",
      "MSE = 0.7619838260746284\n",
      "MSE = 0.7582179856822338\n",
      "MSE = 0.7535894645114701\n",
      "MSE = 0.7532815207391773\n",
      "MSE = 0.7525114019496402\n",
      "MSE = 0.7781583652910753\n",
      "MSE = 0.753160584226129\n",
      "MSE = 0.7517301370160928\n",
      "MSE = 0.7503922299217\n",
      "MSE = 0.7498888451805482\n",
      "MSE = 0.7494045086973435\n",
      "MSE = 0.7493344521392993\n",
      "MSE = 0.7495761506500146\n",
      "MSE = 0.7495205057019675\n",
      "MSE = 0.7527091828771548\n",
      "MSE = 0.749671540021093\n",
      "MSE = 0.7492733683259961\n",
      "MSE = 0.7487982663545615\n",
      "MSE = 0.7479521294705402\n",
      "MSE = 0.7451552162141353\n",
      "MSE = 0.7470985757057704\n",
      "MSE = 0.7471192727537698\n",
      "MSE = 0.7465573155406758\n",
      "MSE = 0.7474544217175062\n",
      "MSE = 0.7466049577790969\n",
      "MSE = 0.7461047203721316\n",
      "MSE = 0.7450985486426712\n",
      "MSE = 0.7457868243576241\n",
      "MSE = 0.7452596729741403\n",
      "MSE = 0.7452848305361568\n",
      "MSE = 0.7452737857299543\n",
      "MSE = 0.7452055379023036\n",
      "MSE = 0.7451228753242577\n",
      "MSE = 0.7427443693710971\n",
      "MSE = 0.7441018595781231\n",
      "MSE = 0.7443573267670519\n",
      "MSE = 0.7442115842810687\n",
      "MSE = 0.7440017042981238\n",
      "MSE = 0.7439062731727898\n",
      "MSE = 0.7438448494518347\n",
      "MSE = 0.7438441090819724\n",
      "MSE = 0.7437169007214699\n",
      "MSE = 0.7436817583718099\n",
      "MSE = 0.747955300716167\n",
      "MSE = 0.7435836445011872\n",
      "MSE = 0.7435710423692791\n",
      "MSE = 0.743510815513793\n",
      "MSE = 0.743501575342757\n",
      "MSE = 0.7420817307857395\n",
      "MSE = 0.7430581210101176\n",
      "MSE = 0.7431133405860852\n",
      "MSE = 0.7431585222102679\n",
      "MSE = 0.7431626803665858\n",
      "MSE = 0.7435053962780622\n",
      "MSE = 0.7432531168721931\n",
      "MSE = 0.7436739617738813\n",
      "MSE = 0.7433744476792223\n",
      "MSE = 0.7432910866126488\n",
      "MSE = 0.7432261269638635\n",
      "MSE = 0.7434612705769228\n",
      "MSE = 0.7432734819499378\n",
      "MSE = 0.7432602766766231\n",
      "MSE = 0.7432757126835859\n",
      "MSE = 0.7433080104146507\n",
      "MSE = 0.7434936053706871\n",
      "MSE = 0.7433921597801184\n",
      "MSE = 0.7435096298895315\n",
      "MSE = 0.7435571876367839\n",
      "MSE = 0.7435702264245343\n",
      "MSE = 0.7430879629811942\n",
      "MSE = 0.7435480967904989\n",
      "MSE = 0.7435580444246698\n",
      "MSE = 0.7435211383774241\n",
      "MSE = 0.743540720074064\n",
      "MSE = 0.743556075887122\n",
      "MSE = 0.7436751065396436\n",
      "MSE = 0.7437949052593452\n",
      "MSE = 0.7439105610926413\n",
      "MSE = 0.7438976944466763\n",
      "MSE = 0.744069709082484\n",
      "MSE = 0.7438773166751976\n",
      "MSE = 0.743968506048042\n",
      "MSE = 0.7439926058974607\n",
      "MSE = 0.744250260994628\n",
      "MSE = 0.744049066334483\n",
      "MSE = 0.7440931977081184\n",
      "MSE = 0.7440893517777074\n",
      "MSE = 0.7441013428061974\n",
      "MSE = 0.7441293789734246\n",
      "MSE = 0.7441676220110672\n",
      "MSE = 0.744267397480469\n",
      "MSE = 0.7442126807636869\n",
      "MSE = 0.7442295184723784\n",
      "MSE = 0.7442303342251135\n",
      "MSE = 0.7442259925791048\n",
      "MSE = 0.7442110374751105\n",
      "MSE = 0.7441272696324213\n",
      "MSE = 0.7441682591584168\n",
      "MSE = 0.7441474774512029\n",
      "MSE = 0.7441543540931607\n",
      "MSE = 0.7441641393636665\n",
      "MSE = 0.7441763006788832\n",
      "MSE = 0.7441849098143224\n",
      "MSE = 0.7441353088804304\n",
      "MSE = 0.7441689565836404\n",
      "MSE = 0.7441712499787437\n",
      "MSE = 0.7441481319107155\n",
      "MSE = 0.744110111284867\n",
      "MSE = 0.7440525554600832\n",
      "MSE = 0.744014506645599\n",
      "MSE = 0.7432463989947024\n",
      "MSE = 0.7439967894695898\n",
      "MSE = 0.7439838979300238\n",
      "MSE = 0.7430381586212537\n",
      "MSE = 0.7439492769465306\n",
      "MSE = 0.7439617753835994\n",
      "MSE = 0.7438934301403994\n",
      "MSE = 0.7437127658261389\n",
      "MSE = 0.7438266421538835\n",
      "MSE = 0.743776921731207\n",
      "MSE = 0.7437424544630742\n",
      "MSE = 0.7436706640778218\n",
      "MSE = 0.7435725678511431\n",
      "MSE = 0.7432625675764692\n",
      "MSE = 0.7418748645904225\n",
      "MSE = 0.7409957982050698\n",
      "MSE = 0.7407980811345553\n",
      "MSE = 0.7362774283958448\n",
      "MSE = 0.7398284567613297\n",
      "MSE = 0.7401361802822125\n",
      "MSE = 0.7377260027360828\n",
      "MSE = 0.7388999235233129\n",
      "MSE = 0.7387829814805952\n",
      "MSE = 0.738087270867196\n",
      "MSE = 0.7370099307399384\n",
      "MSE = 0.7375838787685568\n",
      "MSE = 0.7382152641539622\n",
      "MSE = 0.7372196336307263\n",
      "MSE = 0.7376303574026442\n",
      "MSE = 0.7371533700451479\n",
      "MSE = 0.7371521500352309\n",
      "MSE = 0.7371472865358828\n",
      "MSE = 0.7372421441655075\n",
      "MSE = 0.7372138961275874\n",
      "MSE = 0.7388797000389956\n",
      "MSE = 0.7372375775097003\n",
      "MSE = 0.7372634331465411\n",
      "MSE = 0.7372818589564128\n",
      "MSE = 0.737145247872084\n",
      "MSE = 0.7369264614696741\n",
      "MSE = 0.7365951808590439\n",
      "MSE = 0.7363648043457024\n",
      "MSE = 0.7355203757882057\n",
      "MSE = 0.7360450753578814\n",
      "MSE = 0.7361267955398308\n",
      "MSE = 0.7358663633678226\n",
      "MSE = 0.7364675098935225\n",
      "MSE = 0.7357988511213566\n",
      "MSE = 0.7361921745880626\n",
      "MSE = 0.7357596043410721\n",
      "MSE = 0.735736742441745\n",
      "MSE = 0.7359503072118057\n",
      "MSE = 0.73540064913543\n",
      "MSE = 0.7359386412758474\n",
      "MSE = 0.7360764673459594\n",
      "MSE = 0.7358563612322991\n",
      "MSE = 0.7359677151243079\n",
      "MSE = 0.7359445812346642\n",
      "MSE = 0.7358912748558856\n",
      "MSE = 0.7358079074111133\n",
      "MSE = 0.735646433301608\n",
      "MSE = 0.7360770706977064\n",
      "MSE = 0.7356852214360686\n",
      "MSE = 0.7355245434827343\n",
      "MSE = 0.7353719185464497\n",
      "MSE = 0.7345727634426837\n",
      "MSE = 0.7343138478857896\n",
      "MSE = 0.7342602478066039\n",
      "MSE = 0.7337779549833878\n",
      "MSE = 0.7341849915920161\n",
      "MSE = 0.7342561032516776\n",
      "MSE = 0.7335093425809517\n",
      "MSE = 0.7338203038023684\n",
      "MSE = 0.7337782091139681\n",
      "MSE = 0.7335811710773945\n",
      "MSE = 0.733329374348753\n",
      "MSE = 0.7332198758965334\n",
      "MSE = 0.732814858191422\n",
      "MSE = 0.7323114362641576\n",
      "MSE = 0.7310964893757012\n",
      "MSE = 0.7320443779551756\n",
      "MSE = 0.731758468166445\n",
      "MSE = 0.7318495243958564\n",
      "MSE = 0.731755514962094\n",
      "MSE = 0.7317789447562898\n",
      "MSE = 0.7316936840031341\n",
      "MSE = 0.731430292413513\n",
      "MSE = 0.7310919422204423\n",
      "MSE = 0.7307419612267989\n",
      "MSE = 0.7302734382296046\n",
      "MSE = 0.7305493968082903\n",
      "MSE = 0.7306863276311591\n",
      "MSE = 0.7307840351530007\n",
      "MSE = 0.7306764638086847\n",
      "MSE = 0.7303242353384487\n",
      "MSE = 0.7307350161166069\n",
      "MSE = 0.7304590534724693\n",
      "MSE = 0.7304048167576778\n",
      "MSE = 0.7303282269045446\n",
      "MSE = 0.7302343844311537\n",
      "MSE = 0.7300430046604426\n",
      "MSE = 0.7297757087922935\n",
      "MSE = 0.7463906955957333\n",
      "MSE = 0.7298047453029373\n",
      "MSE = 0.7296810250934996\n",
      "MSE = 0.7296978012888946\n",
      "MSE = 0.7297553036467604\n",
      "MSE = 0.7297941551274123\n",
      "MSE = 0.7299583385998114\n",
      "MSE = 0.7298235510663603\n",
      "MSE = 0.7297341777182771\n",
      "MSE = 0.7296804927818864\n",
      "MSE = 0.7296668481773175\n",
      "MSE = 0.728935283922604\n",
      "MSE = 0.7294270844865255\n",
      "MSE = 0.7290652354633422\n",
      "MSE = 0.7293794131523013\n",
      "MSE = 0.7293625608168178\n",
      "MSE = 0.7294878733416333\n",
      "MSE = 0.7293547548298718\n",
      "MSE = 0.7292222395811531\n",
      "MSE = 0.7290756850903841\n",
      "MSE = 0.7286708771334173\n",
      "MSE = 0.7288960367838124\n",
      "MSE = 0.7288703200909231\n",
      "MSE = 0.7284761436703366\n",
      "MSE = 0.7285953006989281\n",
      "MSE = 0.7286056068088541\n",
      "MSE = 0.7285708970418022\n",
      "MSE = 0.7287304633766423\n",
      "MSE = 0.7286044532269894\n",
      "MSE = 0.7285340219404939\n",
      "MSE = 0.728409927381109\n",
      "MSE = 0.7282500227382486\n",
      "MSE = 0.7282229459802464\n",
      "MSE = 0.7306504569774825\n",
      "MSE = 0.728292624686934\n",
      "MSE = 0.7283130671336399\n",
      "MSE = 0.7283407498084944\n",
      "MSE = 0.7284318629066479\n",
      "MSE = 0.7284446026944497\n",
      "MSE = 0.7284347060095379\n",
      "MSE = 0.7282368622485547\n",
      "MSE = 0.7284166932598696\n",
      "MSE = 0.7283988489166772\n",
      "MSE = 0.7288314761138636\n",
      "MSE = 0.7285288899015332\n",
      "MSE = 0.7284626196719721\n",
      "MSE = 0.7284403233640043\n",
      "MSE = 0.7274981580120092\n",
      "MSE = 0.7282775235820712\n",
      "MSE = 0.7283800300522141\n",
      "MSE = 0.7284745121127091\n",
      "MSE = 0.7284747993164561\n",
      "MSE = 0.7284762117327569\n",
      "MSE = 0.7284700315347051\n",
      "MSE = 0.728428177277772\n",
      "MSE = 0.7284638656061706\n",
      "MSE = 0.7284336109161655\n",
      "MSE = 0.7284185534407016\n",
      "MSE = 0.7284199785917691\n",
      "MSE = 0.7284348787303233\n",
      "MSE = 0.7287923662742025\n",
      "MSE = 0.7284540051710765\n",
      "MSE = 0.7284782728131352\n",
      "MSE = 0.7284888825181501\n",
      "MSE = 0.7284887037476493\n",
      "MSE = 0.7284795431699164\n",
      "MSE = 0.7284501137634498\n",
      "MSE = 0.7283522024409051\n",
      "MSE = 0.7284356776690478\n",
      "MSE = 0.7283918466309243\n",
      "MSE = 0.7283669016596374\n",
      "MSE = 0.7283572849999501\n",
      "MSE = 0.7283523310235971\n",
      "MSE = 0.7283425542065151\n",
      "MSE = 0.7292732588134261\n",
      "MSE = 0.728348554539305\n",
      "MSE = 0.7283381537143288\n",
      "MSE = 0.7283214542474269\n",
      "MSE = 0.7283346206055881\n",
      "MSE = 0.7283366650346779\n",
      "MSE = 0.7283372068765532\n",
      "MSE = 0.7282873143766244\n",
      "MSE = 0.7283288385765985\n",
      "MSE = 0.7283133525019484\n",
      "MSE = 0.728226237325394\n",
      "MSE = 0.7281827654915689\n",
      "MSE = 0.7281257910313644\n",
      "MSE = 0.7280171854549208\n",
      "MSE = 0.7281119970210359\n",
      "MSE = 0.7280780029547035\n",
      "MSE = 0.7279326565447417\n",
      "MSE = 0.7280336904752842\n",
      "MSE = 0.7279777950270904\n",
      "MSE = 0.7279382443802858\n",
      "MSE = 0.7279022781959462\n",
      "MSE = 0.7278634427599473\n",
      "MSE = 0.7278918584245553\n",
      "MSE = 0.7278475203189781\n",
      "MSE = 0.7277987675096798\n",
      "MSE = 0.7277581561185466\n",
      "MSE = 0.7278350597117195\n",
      "MSE = 0.7278299215589619\n",
      "MSE = 0.7278308103360259\n",
      "MSE = 0.7278097335267815\n",
      "MSE = 0.7277881549619895\n",
      "MSE = 0.727768054599673\n",
      "MSE = 0.7277474443331077\n",
      "MSE = 0.7277097235771027\n",
      "MSE = 0.727585030067598\n",
      "MSE = 0.7274472528767969\n",
      "MSE = 0.7274469790426942\n",
      "MSE = 0.7274334072803813\n",
      "MSE = 0.727415800462103\n",
      "MSE = 0.7273330717497236\n",
      "MSE = 0.7273133446311645\n",
      "MSE = 0.7273191934106238\n",
      "MSE = 0.7272574456701494\n",
      "MSE = 0.7272240253632454\n",
      "MSE = 0.7272105174980301\n",
      "MSE = 0.7272033732347875\n",
      "MSE = 0.727012925174329\n",
      "MSE = 0.7271025471930828\n",
      "MSE = 0.727085635284717\n",
      "MSE = 0.7271236973804465\n",
      "MSE = 0.7270740311324974\n",
      "MSE = 0.7270204119649378\n",
      "MSE = 0.7269884448236512\n",
      "MSE = 0.7269255459275841\n",
      "MSE = 0.7269613612163047\n",
      "MSE = 0.7269253007724885\n",
      "MSE = 0.7269079720225338\n",
      "MSE = 0.7268840737602248\n",
      "MSE = 0.7268151357795614\n",
      "MSE = 0.7266500123787022\n",
      "MSE = 0.7267435925326308\n",
      "MSE = 0.7267498045638395\n",
      "MSE = 0.7267477863660672\n",
      "MSE = 0.7267352361831299\n",
      "MSE = 0.726709959687038\n",
      "MSE = 0.7263957298929802\n",
      "MSE = 0.7266583093117831\n",
      "MSE = 0.726647092428914\n",
      "MSE = 0.7266323934386201\n",
      "MSE = 0.7266120089555671\n",
      "MSE = 0.7265787819332263\n",
      "MSE = 0.7267981931717691\n",
      "MSE = 0.7266130601195843\n",
      "MSE = 0.7266528609071495\n",
      "MSE = 0.7264268892324005\n",
      "MSE = 0.7265987249305295\n",
      "MSE = 0.7266685852357574\n",
      "MSE = 0.7266805291366826\n",
      "MSE = 0.7267087558418962\n",
      "MSE = 0.7267177470461996\n",
      "MSE = 0.7267640102889148\n",
      "MSE = 0.7264899222399096\n",
      "MSE = 0.7266731427788554\n",
      "MSE = 0.7266484206343452\n",
      "MSE = 0.7266065533006834\n",
      "MSE = 0.7265875292136046\n",
      "MSE = 0.7265410970171452\n",
      "MSE = 0.7264951153361318\n",
      "MSE = 0.7266408440917165\n",
      "MSE = 0.7265078708120434\n",
      "MSE = 0.7264627990810683\n",
      "MSE = 0.7263111345155476\n",
      "MSE = 0.7261030861120313\n",
      "MSE = 0.7261133094999761\n",
      "MSE = 0.726004802121928\n",
      "MSE = 0.726068900169813\n",
      "MSE = 0.726090009423854\n",
      "MSE = 0.7261153997753977\n",
      "MSE = 0.7261025669534089\n",
      "MSE = 0.72566266856922\n",
      "MSE = 0.7260705945968803\n",
      "MSE = 0.7260235951206719\n",
      "MSE = 0.7259867413792389\n",
      "MSE = 0.7259590467248662\n",
      "MSE = 0.7259367954912094\n",
      "MSE = 0.7259041468438291\n",
      "MSE = 0.7266356300164063\n",
      "MSE = 0.7259125162286794\n",
      "MSE = 0.7259073096659098\n",
      "MSE = 0.7259325129052395\n",
      "MSE = 0.7259657459137119\n",
      "MSE = 0.7260021674483818\n",
      "MSE = 0.7260365708142186\n",
      "MSE = 0.7259317873840152\n",
      "MSE = 0.726022033901629\n",
      "MSE = 0.7260293130574637\n",
      "MSE = 0.725978343847363\n",
      "MSE = 0.7259233027293642\n",
      "MSE = 0.7258856275859351\n",
      "MSE = 0.7258454391924568\n",
      "MSE = 0.7258741373026567\n",
      "MSE = 0.7258379784193377\n",
      "MSE = 0.7257967636205956\n",
      "MSE = 0.7257811724302762\n",
      "MSE = 0.7258138144168688\n",
      "MSE = 0.7258573987076078\n",
      "MSE = 0.7259168528526307\n",
      "MSE = 0.725766904453377\n",
      "MSE = 0.7259046280858666\n",
      "MSE = 0.7259206000427032\n",
      "MSE = 0.7258842330036972\n",
      "MSE = 0.7258523983309582\n",
      "MSE = 0.725819396581321\n",
      "MSE = 0.7257787966724273\n",
      "MSE = 0.7252564649010508\n",
      "MSE = 0.7257350683487035\n",
      "MSE = 0.7257201393982232\n",
      "MSE = 0.7257415638561965\n",
      "MSE = 0.7257746292917922\n",
      "MSE = 0.7257958146867654\n",
      "MSE = 0.7256902667403876\n",
      "MSE = 0.7257837793078724\n",
      "MSE = 0.7258172887710904\n",
      "MSE = 0.7258045298029755\n",
      "MSE = 0.7257705418154147\n",
      "MSE = 0.7256433219663379\n",
      "MSE = 0.7257259664610465\n",
      "MSE = 0.7256652250523651\n",
      "MSE = 0.7256444383733149\n",
      "MSE = 0.7253874512185365\n",
      "MSE = 0.7255352591757795\n",
      "MSE = 0.725575364478698\n",
      "MSE = 0.7256096625685641\n",
      "MSE = 0.7256255173262076\n",
      "MSE = 0.7256399922455852\n",
      "MSE = 0.72586273377615\n",
      "MSE = 0.7256913983934843\n",
      "MSE = 0.7256512378721283\n",
      "MSE = 0.7256138162050589\n",
      "MSE = 0.7255427883124093\n",
      "MSE = 0.7255252477757311\n",
      "MSE = 0.7255329440876136\n",
      "MSE = 0.7255229776280182\n",
      "MSE = 0.7256747249336399\n",
      "MSE = 0.7255792005677887\n",
      "MSE = 0.7255797108251786\n",
      "MSE = 0.7255794207242736\n",
      "MSE = 0.7255784331578181\n",
      "MSE = 0.7255448856589122\n",
      "MSE = 0.7254060559675491\n",
      "MSE = 0.7250958528945618\n",
      "MSE = 0.7253545242626749\n",
      "MSE = 0.7253041862193309\n",
      "MSE = 0.7252687636859598\n",
      "MSE = 0.7252308062146141\n",
      "MSE = 0.7251614220003288\n",
      "MSE = 0.725084476526869\n",
      "MSE = 0.7254642939541007\n",
      "MSE = 0.7251150587591854\n",
      "MSE = 0.7250171532997319\n",
      "MSE = 0.7250104407130273\n",
      "MSE = 0.724983644269786\n",
      "MSE = 0.7249636244528308\n",
      "MSE = 0.7252169242155886\n",
      "MSE = 0.7249978335243011\n",
      "MSE = 0.724940592245151\n",
      "MSE = 0.7249031280625009\n",
      "MSE = 0.724868463103071\n",
      "MSE = 0.7247686989504547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-9be3e81de222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + # Initialize alpha\n\u001b[0m\u001b[1;32m      2\u001b[0m                                    \u001b[0;34m[\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnUsers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnItems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;31m# Initialize beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                    \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnUsers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnItems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Gamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              derivative, args = (labels, 0.00005))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    195\u001b[0m             'maxls': maxls}\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0m\u001b[1;32m    198\u001b[0m                            **opts)\n\u001b[1;32m    199\u001b[0m     d = {'grad': res['jac'],\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-860b9d61e3f3>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(theta, labels, lamb)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-860b9d61e3f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-bc7dce485777>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(user, item)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muserBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mitemBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserGamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemGamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-e349b9aa1f43>\u001b[0m in \u001b[0;36minner\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + # Initialize alpha\n",
    "                                   [0.0001]*(nUsers+nItems) + # Initialize beta\n",
    "                                   [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))], # Gamma\n",
    "                             derivative, args = (labels, 0.00005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "080ecfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFM_gamma_pred(u,i):\n",
    "    if u in userIDs:\n",
    "        b_u = userBiases[u]\n",
    "        g_u = userGamma[u]\n",
    "    else:\n",
    "        b_u = 0\n",
    "        g_u = 0\n",
    "            \n",
    "    if i in itemIDs:\n",
    "        b_i = itemBiases[i]\n",
    "        g_i = itemGamma[i]\n",
    "    else:\n",
    "        b_i = 0\n",
    "        g_i = 0\n",
    "        \n",
    "    if (u in userIDs)&(i in itemIDs):\n",
    "        p = alpha + b_u + b_i + inner(g_u, g_i)\n",
    "    else:\n",
    "        p = alpha + b_u + b_i\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "be6719c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rated_4.txt\", 'w')\n",
    "for l in open(\"stub_Rated.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + '-' + i + ',' + str(LFM_gamma_pred(u,i)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
